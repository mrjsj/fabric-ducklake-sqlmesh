{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae30155-052b-43ac-82b5-14f1c0cdd5bd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c718b85-c581-4b18-b414-0e707ebecf8e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r /lakehouse/default/Files/SQLMesh.Code/requirements.txt\n",
    "import sys\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62b33a-e10f-4fb1-8cf8-893bbac92e07",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Configuration\n",
    "Ensure the key vault secret names match the ones created in the specified key vault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4e01f-779b-4174-b48e-ce57581b25ed",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "KEY_VAULT_NAME=\"kv-ducklake-dev\"\n",
    "\n",
    "FABRIC_WORKSPACE_NAME=\"WS_DuckLake\"\n",
    "FABRIC_LAKEHOUSE_NAME=\"LH_DuckLake\"\n",
    "FABRIC_DUCKLAKE_DATA_PATH=\"DuckLake.Files\"\n",
    "FABRIC_SQLMESH_CODE_PATH=\"SQLMesh.Code\"\n",
    "\n",
    "FABRIC_LAKEHOUSE_TABLES_BASE_PATH=f\"abfss://{FABRIC_WORKSPACE_NAME}@onelake.dfs.fabric.microsoft.com/{FABRIC_LAKEHOUSE_NAME}.Lakehouse/Tables\"\n",
    "\n",
    "# Mapping to the names of the key vault secret names\n",
    "ENV_VARS = {\n",
    "    \"AZURE_CLIENT_ID\": \"DuckLakeClientID\",\n",
    "    \"AZURE_CLIENT_SECRET\": \"DuckLakeClientSecret\",\n",
    "    \"AZURE_TENANT_ID\": \"DuckLakeTenantID\",\n",
    "    \"PG__HOST\": \"DuckLakePGHost\",\n",
    "    \"PG__PORT\": \"DuckLakePGPort\",\n",
    "    \"PG__DATABASE\": \"DuckLakePGDatabase\",\n",
    "    \"PG__USER\": \"DuckLakePGUser\",\n",
    "    \"PG__PASSWORD\": \"DuckLakePGPassword\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8005e9-1c26-4b32-b970-02bb1deecb4d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "for env_var_name, secret_name in ENV_VARS.items():\n",
    "    os.environ[env_var_name] = notebookutils.credentials.getSecret(KEY_VAULT_NAME, secret_name)\n",
    "\n",
    "os.environ[\"FABRIC_WORKSPACE_NAME\"] = FABRIC_WORKSPACE_NAME\n",
    "os.environ[\"FABRIC_LAKEHOUSE_NAME\"] = FABRIC_LAKEHOUSE_NAME\n",
    "os.environ[\"FABRIC_DUCKLAKE_DATA_PATH\"] = FABRIC_DUCKLAKE_DATA_PATH\n",
    "os.environ[\"FABRIC_SQLMESH_CODE_PATH\"] = FABRIC_SQLMESH_CODE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c309ca6c",
   "metadata": {},
   "source": [
    "# Temporary patch until SQLMesh is updated - will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf54ca1-0944-4baa-93a5-6a14e827a32e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "patched = r'''\n",
    "from __future__ import annotations\n",
    "\n",
    "import abc\n",
    "import base64\n",
    "import logging\n",
    "import os\n",
    "import importlib\n",
    "import pathlib\n",
    "import re\n",
    "import typing as t\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "\n",
    "import pydantic\n",
    "from pydantic import Field\n",
    "from pydantic_core import from_json\n",
    "from packaging import version\n",
    "from sqlglot import exp\n",
    "from sqlglot.helper import subclasses\n",
    "\n",
    "from sqlmesh.core import engine_adapter\n",
    "from sqlmesh.core.config.base import BaseConfig\n",
    "from sqlmesh.core.config.common import (\n",
    "    concurrent_tasks_validator,\n",
    "    http_headers_validator,\n",
    "    compile_regex_mapping,\n",
    ")\n",
    "from sqlmesh.core.engine_adapter.shared import CatalogSupport\n",
    "from sqlmesh.core.engine_adapter import EngineAdapter\n",
    "from sqlmesh.utils import debug_mode_enabled, str_to_bool\n",
    "from sqlmesh.utils.errors import ConfigError\n",
    "from sqlmesh.utils.pydantic import (\n",
    "    ValidationInfo,\n",
    "    field_validator,\n",
    "    model_validator,\n",
    "    validation_error_message,\n",
    "    get_concrete_types_from_typehint,\n",
    ")\n",
    "from sqlmesh.utils.aws import validate_s3_uri\n",
    "\n",
    "if t.TYPE_CHECKING:\n",
    "    from sqlmesh.core._typing import Self\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "RECOMMENDED_STATE_SYNC_ENGINES = {\"postgres\", \"gcp_postgres\", \"mysql\", \"mssql\", \"azuresql\"}\n",
    "FORBIDDEN_STATE_SYNC_ENGINES = {\n",
    "    # Do not support row-level operations\n",
    "    \"spark\",\n",
    "    \"trino\",\n",
    "    # Nullable types are problematic\n",
    "    \"clickhouse\",\n",
    "}\n",
    "MOTHERDUCK_TOKEN_REGEX = re.compile(r\"(\\?|\\&)(motherduck_token=)(\\S*)\")\n",
    "\n",
    "\n",
    "def _get_engine_import_validator(\n",
    "    import_name: str, engine_type: str, extra_name: t.Optional[str] = None, decorate: bool = True\n",
    ") -> t.Callable:\n",
    "    extra_name = extra_name or engine_type\n",
    "\n",
    "    def validate(cls: t.Any, data: t.Any) -> t.Any:\n",
    "        check_import = (\n",
    "            str_to_bool(str(data.pop(\"check_import\", True))) if isinstance(data, dict) else True\n",
    "        )\n",
    "        if not check_import:\n",
    "            return data\n",
    "        try:\n",
    "            importlib.import_module(import_name)\n",
    "        except ImportError:\n",
    "            if debug_mode_enabled():\n",
    "                raise\n",
    "\n",
    "            logger.exception(\"Failed to import the engine library\")\n",
    "\n",
    "            raise ConfigError(\n",
    "                f\"Failed to import the '{engine_type}' engine library. This may be due to a missing \"\n",
    "                \"or incompatible installation. Please ensure the required dependency is installed by \"\n",
    "                f'running: `pip install \"sqlmesh[{extra_name}]\"`. For more details, check the logs '\n",
    "                \"in the 'logs/' folder, or rerun the command with the '--debug' flag.\"\n",
    "            )\n",
    "\n",
    "        return data\n",
    "\n",
    "    return model_validator(mode=\"before\")(validate) if decorate else validate\n",
    "\n",
    "\n",
    "class ConnectionConfig(abc.ABC, BaseConfig):\n",
    "    type_: str\n",
    "    concurrent_tasks: int\n",
    "    register_comments: bool\n",
    "    pre_ping: bool\n",
    "    pretty_sql: bool = False\n",
    "\n",
    "    # Whether to share a  single connection across threads or create a new connection per thread.\n",
    "    shared_connection: t.ClassVar[bool] = False\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        \"\"\"keywords that should be passed into the connection\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        \"\"\"The engine adapter for this connection\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        \"\"\"A function that is called to return a connection object for the given Engine Adapter\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _static_connection_kwargs(self) -> t.Dict[str, t.Any]:\n",
    "        \"\"\"The static connection kwargs for this connection\"\"\"\n",
    "        return {}\n",
    "\n",
    "    @property\n",
    "    def _extra_engine_config(self) -> t.Dict[str, t.Any]:\n",
    "        \"\"\"kwargs that are for execution config only\"\"\"\n",
    "        return {}\n",
    "\n",
    "    @property\n",
    "    def _cursor_init(self) -> t.Optional[t.Callable[[t.Any], None]]:\n",
    "        \"\"\"A function that is called to initialize the cursor\"\"\"\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def is_recommended_for_state_sync(self) -> bool:\n",
    "        \"\"\"Whether this engine is recommended for being used as a state sync for production state syncs\"\"\"\n",
    "        return self.type_ in RECOMMENDED_STATE_SYNC_ENGINES\n",
    "\n",
    "    @property\n",
    "    def is_forbidden_for_state_sync(self) -> bool:\n",
    "        \"\"\"Whether this engine is forbidden from being used as a state sync\"\"\"\n",
    "        return self.type_ in FORBIDDEN_STATE_SYNC_ENGINES\n",
    "\n",
    "    @property\n",
    "    def _connection_factory_with_kwargs(self) -> t.Callable[[], t.Any]:\n",
    "        \"\"\"A function that is called to return a connection object for the given Engine Adapter\"\"\"\n",
    "        return partial(\n",
    "            self._connection_factory,\n",
    "            **{\n",
    "                **self._static_connection_kwargs,\n",
    "                **{k: v for k, v in self.dict().items() if k in self._connection_kwargs_keys},\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def connection_validator(self) -> t.Callable[[], None]:\n",
    "        \"\"\"A function that validates the connection configuration\"\"\"\n",
    "        return self.create_engine_adapter().ping\n",
    "\n",
    "    def create_engine_adapter(\n",
    "        self, register_comments_override: bool = False, concurrent_tasks: t.Optional[int] = None\n",
    "    ) -> EngineAdapter:\n",
    "        \"\"\"Returns a new instance of the Engine Adapter.\"\"\"\n",
    "\n",
    "        concurrent_tasks = concurrent_tasks or self.concurrent_tasks\n",
    "        return self._engine_adapter(\n",
    "            self._connection_factory_with_kwargs,\n",
    "            multithreaded=concurrent_tasks > 1,\n",
    "            default_catalog=self.get_catalog(),\n",
    "            cursor_init=self._cursor_init,\n",
    "            register_comments=register_comments_override or self.register_comments,\n",
    "            pre_ping=self.pre_ping,\n",
    "            pretty_sql=self.pretty_sql,\n",
    "            shared_connection=self.shared_connection,\n",
    "            **self._extra_engine_config,\n",
    "        )\n",
    "\n",
    "    def get_catalog(self) -> t.Optional[str]:\n",
    "        \"\"\"The catalog for this connection\"\"\"\n",
    "        if hasattr(self, \"catalog\"):\n",
    "            return self.catalog\n",
    "        if hasattr(self, \"database\"):\n",
    "            return self.database\n",
    "        if hasattr(self, \"db\"):\n",
    "            return self.db\n",
    "        return None\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def _expand_json_strings_to_concrete_types(cls, data: t.Any) -> t.Any:\n",
    "        \"\"\"\n",
    "        There are situations where a connection config class has a field that is some kind of complex type\n",
    "        (eg a list of strings or a dict) but the value is being supplied from a source such as an environment variable\n",
    "\n",
    "        When this happens, the value is supplied as a string rather than a Python object. We need some way\n",
    "        of turning this string into the corresponding Python list or dict.\n",
    "\n",
    "        Rather than doing this piecemeal on every config subclass, this provides a generic implementatation\n",
    "        to identify fields that may be be supplied as JSON strings and handle them transparently\n",
    "        \"\"\"\n",
    "        if data and isinstance(data, dict):\n",
    "            for maybe_json_field_name in cls._get_list_and_dict_field_names():\n",
    "                if (value := data.get(maybe_json_field_name)) and isinstance(value, str):\n",
    "                    # crude JSON check as we dont want to try and parse every string we get\n",
    "                    value = value.strip()\n",
    "                    if value.startswith(\"{\") or value.startswith(\"[\"):\n",
    "                        data[maybe_json_field_name] = from_json(value)\n",
    "\n",
    "        return data\n",
    "\n",
    "    @classmethod\n",
    "    def _get_list_and_dict_field_names(cls) -> t.Set[str]:\n",
    "        field_names = set()\n",
    "        for name, field in cls.model_fields.items():\n",
    "            if field.annotation:\n",
    "                field_types = get_concrete_types_from_typehint(field.annotation)\n",
    "\n",
    "                # check if the field type is something that could concievably be supplied as a json string\n",
    "                if any(ft is t for t in (list, tuple, set, dict) for ft in field_types):\n",
    "                    field_names.add(name)\n",
    "\n",
    "        return field_names\n",
    "\n",
    "\n",
    "class DuckDBAttachOptions(BaseConfig):\n",
    "    type: str\n",
    "    path: str\n",
    "    read_only: bool = False\n",
    "\n",
    "    # DuckLake specific options\n",
    "    data_path: t.Optional[str] = None\n",
    "    encrypted: bool = False\n",
    "    data_inlining_row_limit: t.Optional[int] = None\n",
    "\n",
    "    def to_sql(self, alias: str) -> str:\n",
    "        options = []\n",
    "        # 'duckdb' is actually not a supported type, but we'd like to allow it for\n",
    "        # fully qualified attach options or integration testing, similar to duckdb-dbt\n",
    "        if self.type not in (\"duckdb\", \"motherduck\"):\n",
    "            options.append(f\"TYPE {self.type.upper()}\")\n",
    "        if self.read_only:\n",
    "            options.append(\"READ_ONLY\")\n",
    "\n",
    "        # DuckLake specific options\n",
    "        if self.type == \"ducklake\":\n",
    "            if self.data_path is not None:\n",
    "                options.append(f\"DATA_PATH '{self.data_path}'\")\n",
    "            if self.encrypted:\n",
    "                options.append(\"ENCRYPTED\")\n",
    "            if self.data_inlining_row_limit is not None:\n",
    "                options.append(f\"DATA_INLINING_ROW_LIMIT {self.data_inlining_row_limit}\")\n",
    "\n",
    "        options_sql = f\" ({', '.join(options)})\" if options else \"\"\n",
    "        alias_sql = \"\"\n",
    "        # TODO: Add support for Postgres schema. Currently adding it blocks access to the information_schema\n",
    "\n",
    "        # MotherDuck does not support aliasing\n",
    "        alias_sql = (\n",
    "            f\" AS {alias}\" if not (self.type == \"motherduck\" or self.path.startswith(\"md:\")) else \"\"\n",
    "        )\n",
    "        return f\"ATTACH IF NOT EXISTS '{self.path}'{alias_sql}{options_sql}\"\n",
    "\n",
    "\n",
    "class BaseDuckDBConnectionConfig(ConnectionConfig):\n",
    "    \"\"\"Common configuration for the DuckDB-based connections.\n",
    "\n",
    "    Args:\n",
    "        database: The optional database name. If not specified, the in-memory database will be used.\n",
    "        catalogs: Key is the name of the catalog and value is the path.\n",
    "        extensions: A list of autoloadable extensions to load.\n",
    "        connector_config: A dictionary of configuration to pass into the duckdb connector.\n",
    "        secrets: A list of dictionaries used to generate DuckDB secrets for authenticating with external services (e.g. S3).\n",
    "        file_systems: A list of dictionaries used to register `fsspec` filesystems to the DuckDB cursor.\n",
    "        concurrent_tasks: The maximum number of tasks that can use this connection concurrently.\n",
    "        register_comments: Whether or not to register model comments with the SQL engine.\n",
    "        pre_ping: Whether or not to pre-ping the connection before starting a new transaction to ensure it is still alive.\n",
    "        token: The optional MotherDuck token. If not specified and a MotherDuck path is in the catalog, the user will be prompted to login with their web browser.\n",
    "    \"\"\"\n",
    "\n",
    "    database: t.Optional[str] = None\n",
    "    catalogs: t.Optional[t.Dict[str, t.Union[str, DuckDBAttachOptions]]] = None\n",
    "    extensions: t.List[t.Union[str, t.Dict[str, t.Any]]] = []\n",
    "    connector_config: t.Dict[str, t.Any] = {}\n",
    "    secrets: t.List[t.Dict[str, t.Any]] = []\n",
    "    file_systems: t.List[t.Dict[str, t.Any]] = []\n",
    "\n",
    "    concurrent_tasks: int = 1\n",
    "    register_comments: bool = True\n",
    "    pre_ping: t.Literal[False] = False\n",
    "\n",
    "    token: t.Optional[str] = None\n",
    "\n",
    "    shared_connection: t.ClassVar[bool] = True\n",
    "\n",
    "    _data_file_to_adapter: t.ClassVar[t.Dict[str, EngineAdapter]] = {}\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    def _validate_database_catalogs(cls, data: t.Any) -> t.Any:\n",
    "        if not isinstance(data, dict):\n",
    "            return data\n",
    "\n",
    "        db_path = data.get(\"database\")\n",
    "        if db_path and data.get(\"catalogs\"):\n",
    "            raise ConfigError(\n",
    "                \"Cannot specify both `database` and `catalogs`. Define all your catalogs in `catalogs` and have the first entry be the default catalog\"\n",
    "            )\n",
    "        if isinstance(db_path, str) and db_path.startswith(\"md:\"):\n",
    "            raise ConfigError(\n",
    "                \"Please use connection type 'motherduck' without the `md:` prefix if you want to use a MotherDuck database as the single `database`.\"\n",
    "            )\n",
    "\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        return engine_adapter.DuckDBEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        return {\"database\"}\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        import duckdb\n",
    "\n",
    "        return duckdb.connect\n",
    "\n",
    "    @property\n",
    "    def _cursor_init(self) -> t.Optional[t.Callable[[t.Any], None]]:\n",
    "        \"\"\"A function that is called to initialize the cursor\"\"\"\n",
    "        import duckdb\n",
    "        from duckdb import BinderException\n",
    "\n",
    "        def init(cursor: duckdb.DuckDBPyConnection) -> None:\n",
    "            for extension in self.extensions:\n",
    "                extension = extension if isinstance(extension, dict) else {\"name\": extension}\n",
    "\n",
    "                install_command = f\"INSTALL {extension['name']}\"\n",
    "\n",
    "                if extension.get(\"repository\"):\n",
    "                    install_command = f\"{install_command} FROM {extension['repository']}\"\n",
    "\n",
    "                if extension.get(\"force_install\"):\n",
    "                    install_command = f\"FORCE {install_command}\"\n",
    "\n",
    "                try:\n",
    "                    cursor.execute(install_command)\n",
    "                    cursor.execute(f\"LOAD {extension['name']}\")\n",
    "                except Exception as e:\n",
    "                    raise ConfigError(f\"Failed to load extension {extension['name']}: {e}\")\n",
    "\n",
    "            for field, setting in self.connector_config.items():\n",
    "                try:\n",
    "                    cursor.execute(f\"SET {field} = '{setting}'\")\n",
    "                except Exception as e:\n",
    "                    raise ConfigError(f\"Failed to set connector config {field} to {setting}: {e}\")\n",
    "\n",
    "            if self.secrets:\n",
    "                duckdb_version = duckdb.__version__\n",
    "                if version.parse(duckdb_version) < version.parse(\"0.10.0\"):\n",
    "                    from sqlmesh.core.console import get_console\n",
    "\n",
    "                    get_console().log_warning(\n",
    "                        f\"DuckDB version {duckdb_version} does not support secrets-based authentication (requires 0.10.0 or later).\\n\"\n",
    "                        \"To use secrets, please upgrade DuckDB. For older versions, configure legacy authentication via `connector_config`.\\n\"\n",
    "                        \"More info: https://duckdb.org/docs/stable/extensions/httpfs/s3api_legacy_authentication.html\"\n",
    "                    )\n",
    "                else:\n",
    "                    for secrets in self.secrets:\n",
    "                        secret_settings: t.List[str] = []\n",
    "                        for field, setting in secrets.items():\n",
    "                            secret_settings.append(f\"{field} '{setting}'\")\n",
    "                        if secret_settings:\n",
    "                            secret_clause = \", \".join(secret_settings)\n",
    "                            try:\n",
    "                                cursor.execute(f\"CREATE SECRET ({secret_clause});\")\n",
    "                            except Exception as e:\n",
    "                                raise ConfigError(f\"Failed to create secret: {e}\")\n",
    "\n",
    "            if self.file_systems:\n",
    "                from fsspec import filesystem  # type: ignore\n",
    "\n",
    "                for file_system in self.file_systems:\n",
    "                    options = file_system.copy()\n",
    "                    fs = file_system.pop(\"fs\")\n",
    "                    fs = filesystem(protocol, **options)\n",
    "                    cursor.register_filesystem(fs)\n",
    "\n",
    "            for i, (alias, path_options) in enumerate(\n",
    "                (getattr(self, \"catalogs\", None) or {}).items()\n",
    "            ):\n",
    "                # we parse_identifier and generate to ensure that `alias` has exactly one set of quotes\n",
    "                # regardless of whether it comes in quoted or not\n",
    "                alias = exp.parse_identifier(alias, dialect=\"duckdb\").sql(\n",
    "                    identify=True, dialect=\"duckdb\"\n",
    "                )\n",
    "                try:\n",
    "                    if isinstance(path_options, DuckDBAttachOptions):\n",
    "                        query = path_options.to_sql(alias)\n",
    "                    else:\n",
    "                        query = f\"ATTACH IF NOT EXISTS '{path_options}'\"\n",
    "                        if not path_options.startswith(\"md:\"):\n",
    "                            query += f\" AS {alias}\"\n",
    "                    cursor.execute(query)\n",
    "                except BinderException as e:\n",
    "                    # If a user tries to create a catalog pointing at `:memory:` and with the name `memory`\n",
    "                    # then we don't want to raise since this happens by default. They are just doing this to\n",
    "                    # set it as the default catalog.\n",
    "                    # If a user tried to attach a MotherDuck database/share which has already by attached via\n",
    "                    # `ATTACH 'md:'`, then we don't want to raise since this is expected.\n",
    "                    if (\n",
    "                        not (\n",
    "                            'database with name \"memory\" already exists' in str(e)\n",
    "                            and path_options == \":memory:\"\n",
    "                        )\n",
    "                        and f\"\"\"database with name \"{path_options.path.replace(\"md:\", \"\")}\" already exists\"\"\"\n",
    "                        not in str(e)\n",
    "                    ):\n",
    "                        raise e\n",
    "                if i == 0 and not getattr(self, \"database\", None):\n",
    "                    cursor.execute(f\"USE {alias}\")\n",
    "\n",
    "        return init\n",
    "\n",
    "    def create_engine_adapter(\n",
    "        self, register_comments_override: bool = False, concurrent_tasks: t.Optional[int] = None\n",
    "    ) -> EngineAdapter:\n",
    "        \"\"\"Checks if another engine adapter has already been created that shares a catalog that points to the same data\n",
    "        file. If so, it uses that same adapter instead of creating a new one. As a result, any additional configuration\n",
    "        associated with the new adapter will be ignored.\"\"\"\n",
    "        data_files = set((self.catalogs or {}).values())\n",
    "        if self.database:\n",
    "            if isinstance(self, MotherDuckConnectionConfig):\n",
    "                data_files.add(\n",
    "                    f\"md:{self.database}\"\n",
    "                    + (f\"?motherduck_token={self.token}\" if self.token else \"\")\n",
    "                )\n",
    "            else:\n",
    "                data_files.add(self.database)\n",
    "        data_files.discard(\":memory:\")\n",
    "        for data_file in data_files:\n",
    "            key = data_file if isinstance(data_file, str) else data_file.path\n",
    "            adapter = BaseDuckDBConnectionConfig._data_file_to_adapter.get(key)\n",
    "            if adapter is not None:\n",
    "                logger.info(\n",
    "                    f\"Using existing DuckDB adapter due to overlapping data file: {self._mask_motherduck_token(key)}\"\n",
    "                )\n",
    "                return adapter\n",
    "\n",
    "        if data_files:\n",
    "            masked_files = {\n",
    "                self._mask_motherduck_token(file if isinstance(file, str) else file.path)\n",
    "                for file in data_files\n",
    "            }\n",
    "            logger.info(f\"Creating new DuckDB adapter for data files: {masked_files}\")\n",
    "        else:\n",
    "            logger.info(\"Creating new DuckDB adapter for in-memory database\")\n",
    "        adapter = super().create_engine_adapter(\n",
    "            register_comments_override, concurrent_tasks=concurrent_tasks\n",
    "        )\n",
    "        for data_file in data_files:\n",
    "            key = data_file if isinstance(data_file, str) else data_file.path\n",
    "            BaseDuckDBConnectionConfig._data_file_to_adapter[key] = adapter\n",
    "        return adapter\n",
    "\n",
    "    def get_catalog(self) -> t.Optional[str]:\n",
    "        if self.database:\n",
    "            # Remove `:` from the database name in order to handle if `:memory:` is passed in\n",
    "            return pathlib.Path(self.database.replace(\":memory:\", \"memory\")).stem\n",
    "        if self.catalogs:\n",
    "            return list(self.catalogs)[0]\n",
    "        return None\n",
    "\n",
    "    def _mask_motherduck_token(self, string: str) -> str:\n",
    "        return MOTHERDUCK_TOKEN_REGEX.sub(\n",
    "            lambda m: f\"{m.group(1)}{m.group(2)}{'*' * len(m.group(3))}\", string\n",
    "        )\n",
    "\n",
    "\n",
    "class MotherDuckConnectionConfig(BaseDuckDBConnectionConfig):\n",
    "    \"\"\"Configuration for the MotherDuck connection.\"\"\"\n",
    "\n",
    "    type_: t.Literal[\"motherduck\"] = Field(alias=\"type\", default=\"motherduck\")\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        return set()\n",
    "\n",
    "    @property\n",
    "    def _static_connection_kwargs(self) -> t.Dict[str, t.Any]:\n",
    "        \"\"\"kwargs that are for execution config only\"\"\"\n",
    "        from sqlmesh import __version__\n",
    "\n",
    "        custom_user_agent_config = {\"custom_user_agent\": f\"SQLMesh/{__version__}\"}\n",
    "        connection_str = \"md:\"\n",
    "        if self.database:\n",
    "            # Attach single MD database instead of all databases on the account\n",
    "            connection_str += f\"{self.database}?attach_mode=single\"\n",
    "        if self.token:\n",
    "            connection_str += f\"{'&' if self.database else '?'}motherduck_token={self.token}\"\n",
    "        return {\"database\": connection_str, \"config\": custom_user_agent_config}\n",
    "\n",
    "\n",
    "class DuckDBConnectionConfig(BaseDuckDBConnectionConfig):\n",
    "    \"\"\"Configuration for the DuckDB connection.\"\"\"\n",
    "\n",
    "    type_: t.Literal[\"duckdb\"] = Field(alias=\"type\", default=\"duckdb\")\n",
    "\n",
    "\n",
    "class SnowflakeConnectionConfig(ConnectionConfig):\n",
    "    \"\"\"Configuration for the Snowflake connection.\n",
    "\n",
    "    Args:\n",
    "        account: The Snowflake account name.\n",
    "        user: The Snowflake username.\n",
    "        password: The Snowflake password.\n",
    "        warehouse: The optional warehouse name.\n",
    "        database: The optional database name.\n",
    "        role: The optional role name.\n",
    "        concurrent_tasks: The maximum number of tasks that can use this connection concurrently.\n",
    "        authenticator: The optional authenticator name. Defaults to username/password authentication (\"snowflake\").\n",
    "                       Options: https://github.com/snowflakedb/snowflake-connector-python/blob/e937591356c067a77f34a0a42328907fda792c23/src/snowflake/connector/network.py#L178-L183\n",
    "        token: The optional oauth access token to use for authentication when authenticator is set to \"oauth\".\n",
    "        private_key: The optional private key to use for authentication. Key can be Base64-encoded DER format (representing the key bytes), a plain-text PEM format, or bytes (Python config only). https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-connect#using-key-pair-authentication-key-pair-rotation\n",
    "        private_key_path: The optional path to the private key to use for authentication. This would be used instead of `private_key`.\n",
    "        private_key_passphrase: The optional passphrase to use to decrypt `private_key` or `private_key_path`. Keys can be created without encryption so only provide this if needed.\n",
    "        register_comments: Whether or not to register model comments with the SQL engine.\n",
    "        pre_ping: Whether or not to pre-ping the connection before starting a new transaction to ensure it is still alive.\n",
    "        session_parameters: The optional session parameters to set for the connection.\n",
    "        host: Host address for the connection.\n",
    "        port: Port for the connection.\n",
    "    \"\"\"\n",
    "\n",
    "    account: str\n",
    "    user: t.Optional[str] = None\n",
    "    password: t.Optional[str] = None\n",
    "    warehouse: t.Optional[str] = None\n",
    "    database: t.Optional[str] = None\n",
    "    role: t.Optional[str] = None\n",
    "    authenticator: t.Optional[str] = None\n",
    "    token: t.Optional[str] = None\n",
    "    host: t.Optional[str] = None\n",
    "    port: t.Optional[int] = None\n",
    "    application: t.Literal[\"Tobiko_SQLMesh\"] = \"Tobiko_SQLMesh\"\n",
    "\n",
    "    # Private Key Auth\n",
    "    private_key: t.Optional[t.Union[str, bytes]] = None\n",
    "    private_key_path: t.Optional[str] = None\n",
    "    private_key_passphrase: t.Optional[str] = None\n",
    "\n",
    "    concurrent_tasks: int = 4\n",
    "    register_comments: bool = True\n",
    "    pre_ping: bool = False\n",
    "\n",
    "    session_parameters: t.Optional[dict] = None\n",
    "\n",
    "    type_: t.Literal[\"snowflake\"] = Field(alias=\"type\", default=\"snowflake\")\n",
    "\n",
    "    _concurrent_tasks_validator = concurrent_tasks_validator\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    def _validate_authenticator(cls, data: t.Any) -> t.Any:\n",
    "        if not isinstance(data, dict):\n",
    "            return data\n",
    "\n",
    "        from snowflake.connector.network import DEFAULT_AUTHENTICATOR, OAUTH_AUTHENTICATOR\n",
    "\n",
    "        auth = data.get(\"authenticator\")\n",
    "        auth = auth.upper() if auth else DEFAULT_AUTHENTICATOR\n",
    "        user = data.get(\"user\")\n",
    "        password = data.get(\"password\")\n",
    "        data[\"private_key\"] = cls._get_private_key(data, auth)  # type: ignore\n",
    "\n",
    "        if (\n",
    "            auth == DEFAULT_AUTHENTICATOR\n",
    "            and not data.get(\"private_key\")\n",
    "            and (not user or not password)\n",
    "        ):\n",
    "            raise ConfigError(\"User and password must be provided if using default authentication\")\n",
    "\n",
    "        if auth == OAUTH_AUTHENTICATOR and not data.get(\"token\"):\n",
    "            raise ConfigError(\"Token must be provided if using oauth authentication\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    _engine_import_validator = _get_engine_import_validator(\n",
    "        \"snowflake.connector.network\", \"snowflake\"\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def _get_private_key(cls, values: t.Dict[str, t.Optional[str]], auth: str) -> t.Optional[bytes]:\n",
    "        \"\"\"\n",
    "        source: https://github.com/dbt-labs/dbt-snowflake/blob/0374b4ec948982f2ac8ec0c95d53d672ad19e09c/dbt/adapters/snowflake/connections.py#L247C5-L285C1\n",
    "\n",
    "        Overall code change: Use local variables instead of class attributes + Validation\n",
    "        \"\"\"\n",
    "        # Start custom code\n",
    "        from cryptography.hazmat.backends import default_backend\n",
    "        from cryptography.hazmat.primitives import serialization\n",
    "        from snowflake.connector.network import (\n",
    "            DEFAULT_AUTHENTICATOR,\n",
    "            KEY_PAIR_AUTHENTICATOR,\n",
    "        )\n",
    "\n",
    "        private_key = values.get(\"private_key\")\n",
    "        private_key_path = values.get(\"private_key_path\")\n",
    "        private_key_passphrase = values.get(\"private_key_passphrase\")\n",
    "        user = values.get(\"user\")\n",
    "        password = values.get(\"password\")\n",
    "        auth = auth if auth and auth != DEFAULT_AUTHENTICATOR else KEY_PAIR_AUTHENTICATOR\n",
    "\n",
    "        if not private_key and not private_key_path:\n",
    "            return None\n",
    "        if private_key and private_key_path:\n",
    "            raise ConfigError(\"Cannot specify both `private_key` and `private_key_path`\")\n",
    "        if auth != KEY_PAIR_AUTHENTICATOR:\n",
    "            raise ConfigError(\n",
    "                f\"Private key or private key path can only be provided when using {KEY_PAIR_AUTHENTICATOR} authentication\"\n",
    "            )\n",
    "        if not user:\n",
    "            raise ConfigError(\n",
    "                f\"User must be provided when using {KEY_PAIR_AUTHENTICATOR} authentication\"\n",
    "            )\n",
    "        if password:\n",
    "            raise ConfigError(\n",
    "                f\"Password cannot be provided when using {KEY_PAIR_AUTHENTICATOR} authentication\"\n",
    "            )\n",
    "\n",
    "        if isinstance(private_key, bytes):\n",
    "            return private_key\n",
    "        # End Custom Code\n",
    "\n",
    "        if private_key_passphrase:\n",
    "            encoded_passphrase = private_key_passphrase.encode()\n",
    "        else:\n",
    "            encoded_passphrase = None\n",
    "\n",
    "        if private_key:\n",
    "            if private_key.startswith(\"-\"):\n",
    "                p_key = serialization.load_pem_private_key(\n",
    "                    data=bytes(private_key, \"utf-8\"),\n",
    "                    password=encoded_passphrase,\n",
    "                    backend=default_backend(),\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                p_key = serialization.load_der_private_key(\n",
    "                    data=base64.b64decode(private_key),\n",
    "                    password=encoded_passphrase,\n",
    "                    backend=default_backend(),\n",
    "                )\n",
    "\n",
    "        elif private_key_path:\n",
    "            with open(private_key_path, \"rb\") as key:\n",
    "                p_key = serialization.load_pem_private_key(\n",
    "                    key.read(), password=encoded_passphrase, backend=default_backend()\n",
    "                )\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        return p_key.private_bytes(\n",
    "            encoding=serialization.Encoding.DER,\n",
    "            format=serialization.PrivateFormat.PKCS8,\n",
    "            encryption_algorithm=serialization.NoEncryption(),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        return {\n",
    "            \"user\",\n",
    "            \"password\",\n",
    "            \"account\",\n",
    "            \"warehouse\",\n",
    "            \"database\",\n",
    "            \"role\",\n",
    "            \"authenticator\",\n",
    "            \"token\",\n",
    "            \"private_key\",\n",
    "            \"session_parameters\",\n",
    "            \"application\",\n",
    "            \"host\",\n",
    "            \"port\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        return engine_adapter.SnowflakeEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _static_connection_kwargs(self) -> t.Dict[str, t.Any]:\n",
    "        return {\"autocommit\": False}\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        from snowflake import connector\n",
    "\n",
    "        return connector.connect\n",
    "\n",
    "\n",
    "class DatabricksConnectionConfig(ConnectionConfig):\n",
    "    \"\"\"\n",
    "    Databricks connection that uses the SQL connector for SQL models and then Databricks Connect for Dataframe operations\n",
    "\n",
    "    Arg Source: https://github.com/databricks/databricks-sql-python/blob/main/src/databricks/sql/client.py#L39\n",
    "    OAuth ref: https://docs.databricks.com/en/dev-tools/python-sql-connector.html#oauth-machine-to-machine-m2m-authentication\n",
    "\n",
    "    Args:\n",
    "        server_hostname: Databricks instance host name.\n",
    "        http_path: Http path either to a DBSQL endpoint (e.g. /sql/1.0/endpoints/1234567890abcdef)\n",
    "            or to a DBR interactive cluster (e.g. /sql/protocolv1/o/1234567890123456/1234-123456-slid123)\n",
    "        access_token: Http Bearer access token, e.g. Databricks Personal Access Token.\n",
    "        auth_type: Set to 'databricks-oauth' or 'azure-oauth' to trigger OAuth (or dont set at all to use `access_token`)\n",
    "        oauth_client_id: Client ID to use when auth_type is set to one of the 'oauth' types\n",
    "        oauth_client_secret: Client Secret to use when auth_type is set to one of the 'oauth' types\n",
    "        catalog: Default catalog to use for SQL models. Defaults to None which means it will use the default set in\n",
    "            the Databricks cluster (most likely `hive_metastore`).\n",
    "        http_headers: An optional list of (k, v) pairs that will be set as Http headers on every request\n",
    "        session_configuration: An optional dictionary of Spark session parameters.\n",
    "            Execute the SQL command `SET -v` to get a full list of available commands.\n",
    "        databricks_connect_server_hostname: The hostname to use when establishing a connecting using Databricks Connect.\n",
    "            Defaults to the `server_hostname` value.\n",
    "        databricks_connect_access_token: The access token to use when establishing a connecting using Databricks Connect.\n",
    "            Defaults to the `access_token` value.\n",
    "        databricks_connect_cluster_id: The cluster id to use when establishing a connecting using Databricks Connect.\n",
    "            Defaults to deriving the cluster id from the `http_path` value.\n",
    "        force_databricks_connect: Force all queries to run using Databricks Connect instead of the SQL connector.\n",
    "        disable_databricks_connect: Even if databricks connect is installed, do not use it.\n",
    "        disable_spark_session: Do not use SparkSession if it is available (like when running in a notebook).\n",
    "        pre_ping: Whether or not to pre-ping the connection before starting a new transaction to ensure it is still alive.\n",
    "    \"\"\"\n",
    "\n",
    "    server_hostname: t.Optional[str] = None\n",
    "    http_path: t.Optional[str] = None\n",
    "    access_token: t.Optional[str] = None\n",
    "    auth_type: t.Optional[str] = None\n",
    "    oauth_client_id: t.Optional[str] = None\n",
    "    oauth_client_secret: t.Optional[str] = None\n",
    "    catalog: t.Optional[str] = None\n",
    "    http_headers: t.Optional[t.List[t.Tuple[str, str]]] = None\n",
    "    session_configuration: t.Optional[t.Dict[str, t.Any]] = None\n",
    "    databricks_connect_server_hostname: t.Optional[str] = None\n",
    "    databricks_connect_access_token: t.Optional[str] = None\n",
    "    databricks_connect_cluster_id: t.Optional[str] = None\n",
    "    databricks_connect_use_serverless: bool = False\n",
    "    force_databricks_connect: bool = False\n",
    "    disable_databricks_connect: bool = False\n",
    "    disable_spark_session: bool = False\n",
    "\n",
    "    concurrent_tasks: int = 1\n",
    "    register_comments: bool = True\n",
    "    pre_ping: t.Literal[False] = False\n",
    "\n",
    "    type_: t.Literal[\"databricks\"] = Field(alias=\"type\", default=\"databricks\")\n",
    "\n",
    "    _concurrent_tasks_validator = concurrent_tasks_validator\n",
    "    _http_headers_validator = http_headers_validator\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    def _databricks_connect_validator(cls, data: t.Any) -> t.Any:\n",
    "        # SQLQueryContextLogger will output any error SQL queries even if they are in a try/except block.\n",
    "        # Disabling this allows SQLMesh to determine what should be shown to the user.\n",
    "        # Ex: We describe a table to see if it exists and therefore that execution can fail but we don't need to show\n",
    "        # the user since it is expected if the table doesn't exist. Without this change the user would see the error.\n",
    "        logging.getLogger(\"SQLQueryContextLogger\").setLevel(logging.CRITICAL)\n",
    "\n",
    "        if not isinstance(data, dict):\n",
    "            return data\n",
    "\n",
    "        from sqlmesh.core.engine_adapter.databricks import DatabricksEngineAdapter\n",
    "\n",
    "        if DatabricksEngineAdapter.can_access_spark_session(\n",
    "            bool(data.get(\"disable_spark_session\"))\n",
    "        ):\n",
    "            return data\n",
    "\n",
    "        databricks_connect_use_serverless = data.get(\"databricks_connect_use_serverless\")\n",
    "        server_hostname, http_path, access_token, auth_type = (\n",
    "            data.get(\"server_hostname\"),\n",
    "            data.get(\"http_path\"),\n",
    "            data.get(\"access_token\"),\n",
    "            data.get(\"auth_type\"),\n",
    "        )\n",
    "\n",
    "        if (not server_hostname or not http_path or not access_token) and (\n",
    "            not databricks_connect_use_serverless and not auth_type\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"`server_hostname`, `http_path`, and `access_token` are required for Databricks connections when not running in a notebook\"\n",
    "            )\n",
    "        if (\n",
    "            databricks_connect_use_serverless\n",
    "            and not server_hostname\n",
    "            and not data.get(\"databricks_connect_server_hostname\")\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"`server_hostname` or `databricks_connect_server_hostname` is required when `databricks_connect_use_serverless` is set\"\n",
    "            )\n",
    "        if DatabricksEngineAdapter.can_access_databricks_connect(\n",
    "            bool(data.get(\"disable_databricks_connect\"))\n",
    "        ):\n",
    "            if not data.get(\"databricks_connect_access_token\"):\n",
    "                data[\"databricks_connect_access_token\"] = access_token\n",
    "            if not data.get(\"databricks_connect_server_hostname\"):\n",
    "                data[\"databricks_connect_server_hostname\"] = f\"https://{server_hostname}\"\n",
    "            if not databricks_connect_use_serverless and not data.get(\n",
    "                \"databricks_connect_cluster_id\"\n",
    "            ):\n",
    "                if t.TYPE_CHECKING:\n",
    "                    assert http_path is not None\n",
    "                data[\"databricks_connect_cluster_id\"] = http_path.split(\"/\")[-1]\n",
    "\n",
    "        if auth_type:\n",
    "            from databricks.sql.auth.auth import AuthType\n",
    "\n",
    "            all_data = [m.value for m in AuthType]\n",
    "            if auth_type not in all_data:\n",
    "                raise ValueError(\n",
    "                    f\"`auth_type` {auth_type} does not match a valid option: {all_data}\"\n",
    "                )\n",
    "\n",
    "            client_id = data.get(\"oauth_client_id\")\n",
    "            client_secret = data.get(\"oauth_client_secret\")\n",
    "\n",
    "            if client_secret and not client_id:\n",
    "                raise ValueError(\n",
    "                    \"`oauth_client_id` is required when `oauth_client_secret` is specified\"\n",
    "                )\n",
    "\n",
    "            if not http_path:\n",
    "                raise ValueError(\"`http_path` is still required when using `auth_type`\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    _engine_import_validator = _get_engine_import_validator(\"databricks\", \"databricks\")\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        if self.use_spark_session_only:\n",
    "            return set()\n",
    "        return {\n",
    "            \"server_hostname\",\n",
    "            \"http_path\",\n",
    "            \"access_token\",\n",
    "            \"http_headers\",\n",
    "            \"session_configuration\",\n",
    "            \"catalog\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[engine_adapter.DatabricksEngineAdapter]:\n",
    "        return engine_adapter.DatabricksEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _extra_engine_config(self) -> t.Dict[str, t.Any]:\n",
    "        return {\n",
    "            k: v\n",
    "            for k, v in self.dict().items()\n",
    "            if k.startswith(\"databricks_connect_\")\n",
    "            or k in (\"catalog\", \"disable_databricks_connect\", \"disable_spark_session\")\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def use_spark_session_only(self) -> bool:\n",
    "        from sqlmesh.core.engine_adapter.databricks import DatabricksEngineAdapter\n",
    "\n",
    "        return (\n",
    "            DatabricksEngineAdapter.can_access_spark_session(self.disable_spark_session)\n",
    "            or self.force_databricks_connect\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        if self.use_spark_session_only:\n",
    "            from sqlmesh.engines.spark.db_api.spark_session import connection\n",
    "\n",
    "            return connection\n",
    "\n",
    "        from databricks import sql  # type: ignore\n",
    "\n",
    "        return sql.connect\n",
    "\n",
    "    @property\n",
    "    def _static_connection_kwargs(self) -> t.Dict[str, t.Any]:\n",
    "        from sqlmesh.core.engine_adapter.databricks import DatabricksEngineAdapter\n",
    "\n",
    "        if not self.use_spark_session_only:\n",
    "            conn_kwargs: t.Dict[str, t.Any] = {\n",
    "                \"_user_agent_entry\": \"sqlmesh\",\n",
    "            }\n",
    "\n",
    "            if self.auth_type and \"oauth\" in self.auth_type:\n",
    "                # there are two types of oauth: User-to-Machine (U2M) and Machine-to-Machine (M2M)\n",
    "                if self.oauth_client_secret:\n",
    "                    # if a client_secret exists, then a client_id also exists and we are using M2M\n",
    "                    # ref: https://docs.databricks.com/en/dev-tools/python-sql-connector.html#oauth-machine-to-machine-m2m-authentication\n",
    "                    # ref: https://github.com/databricks/databricks-sql-python/blob/main/examples/m2m_oauth.py\n",
    "                    from databricks.sdk.core import oauth_service_principal, Config\n",
    "\n",
    "                    config = Config(\n",
    "                        host=f\"https://{self.server_hostname}\",\n",
    "                        client_id=self.oauth_client_id,\n",
    "                        client_secret=self.oauth_client_secret,\n",
    "                    )\n",
    "                    conn_kwargs[\"credentials_provider\"] = lambda: oauth_service_principal(config)\n",
    "                else:\n",
    "                    # if auth_type is set to an 'oauth' type but no client_id/secret are set, then we are using U2M\n",
    "                    # ref: https://docs.databricks.com/en/dev-tools/python-sql-connector.html#oauth-user-to-machine-u2m-authentication\n",
    "                    conn_kwargs[\"auth_type\"] = self.auth_type\n",
    "\n",
    "            return conn_kwargs\n",
    "\n",
    "        if DatabricksEngineAdapter.can_access_spark_session(self.disable_spark_session):\n",
    "            from pyspark.sql import SparkSession\n",
    "\n",
    "            return dict(\n",
    "                spark=SparkSession.getActiveSession(),\n",
    "                catalog=self.catalog,\n",
    "            )\n",
    "\n",
    "        from databricks.connect import DatabricksSession\n",
    "\n",
    "        if t.TYPE_CHECKING:\n",
    "            assert self.databricks_connect_server_hostname is not None\n",
    "            assert self.databricks_connect_access_token is not None\n",
    "\n",
    "        if self.databricks_connect_use_serverless:\n",
    "            builder = DatabricksSession.builder.remote(\n",
    "                host=self.databricks_connect_server_hostname,\n",
    "                token=self.databricks_connect_access_token,\n",
    "                serverless=True,\n",
    "            )\n",
    "        else:\n",
    "            if t.TYPE_CHECKING:\n",
    "                assert self.databricks_connect_cluster_id is not None\n",
    "            builder = DatabricksSession.builder.remote(\n",
    "                host=self.databricks_connect_server_hostname,\n",
    "                token=self.databricks_connect_access_token,\n",
    "                cluster_id=self.databricks_connect_cluster_id,\n",
    "            )\n",
    "\n",
    "        return dict(\n",
    "            spark=builder.userAgent(\"sqlmesh\").getOrCreate(),\n",
    "            catalog=self.catalog,\n",
    "        )\n",
    "\n",
    "\n",
    "class BigQueryConnectionMethod(str, Enum):\n",
    "    OAUTH = \"oauth\"\n",
    "    OAUTH_SECRETS = \"oauth-secrets\"\n",
    "    SERVICE_ACCOUNT = \"service-account\"\n",
    "    SERVICE_ACCOUNT_JSON = \"service-account-json\"\n",
    "\n",
    "\n",
    "class BigQueryPriority(str, Enum):\n",
    "    BATCH = \"batch\"\n",
    "    INTERACTIVE = \"interactive\"\n",
    "\n",
    "    @property\n",
    "    def is_batch(self) -> bool:\n",
    "        return self == self.BATCH\n",
    "\n",
    "    @property\n",
    "    def is_interactive(self) -> bool:\n",
    "        return self == self.INTERACTIVE\n",
    "\n",
    "    @property\n",
    "    def bigquery_constant(self) -> str:\n",
    "        from google.cloud.bigquery import QueryPriority\n",
    "\n",
    "        if self.is_batch:\n",
    "            return QueryPriority.BATCH\n",
    "        return QueryPriority.INTERACTIVE\n",
    "\n",
    "\n",
    "class BigQueryConnectionConfig(ConnectionConfig):\n",
    "    \"\"\"\n",
    "    BigQuery Connection Configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    method: BigQueryConnectionMethod = BigQueryConnectionMethod.OAUTH\n",
    "\n",
    "    project: t.Optional[str] = None\n",
    "    execution_project: t.Optional[str] = None\n",
    "    quota_project: t.Optional[str] = None\n",
    "    location: t.Optional[str] = None\n",
    "    # Keyfile Auth\n",
    "    keyfile: t.Optional[str] = None\n",
    "    keyfile_json: t.Optional[t.Dict[str, t.Any]] = None\n",
    "    # Oath Secret Auth\n",
    "    token: t.Optional[str] = None\n",
    "    refresh_token: t.Optional[str] = None\n",
    "    client_id: t.Optional[str] = None\n",
    "    client_secret: t.Optional[str] = None\n",
    "    token_uri: t.Optional[str] = None\n",
    "    scopes: t.Tuple[str, ...] = (\"https://www.googleapis.com/auth/bigquery\",)\n",
    "    impersonated_service_account: t.Optional[str] = None\n",
    "    # Extra Engine Config\n",
    "    job_creation_timeout_seconds: t.Optional[int] = None\n",
    "    job_execution_timeout_seconds: t.Optional[int] = None\n",
    "    job_retries: t.Optional[int] = 1\n",
    "    job_retry_deadline_seconds: t.Optional[int] = None\n",
    "    priority: t.Optional[BigQueryPriority] = None\n",
    "    maximum_bytes_billed: t.Optional[int] = None\n",
    "\n",
    "    concurrent_tasks: int = 1\n",
    "    register_comments: bool = True\n",
    "    pre_ping: t.Literal[False] = False\n",
    "\n",
    "    type_: t.Literal[\"bigquery\"] = Field(alias=\"type\", default=\"bigquery\")\n",
    "\n",
    "    _engine_import_validator = _get_engine_import_validator(\"google.cloud.bigquery\", \"bigquery\")\n",
    "\n",
    "    @field_validator(\"execution_project\")\n",
    "    def validate_execution_project(\n",
    "        cls,\n",
    "        v: t.Optional[str],\n",
    "        info: ValidationInfo,\n",
    "    ) -> t.Optional[str]:\n",
    "        if v and not info.data.get(\"project\"):\n",
    "            raise ConfigError(\n",
    "                \"If the `execution_project` field is specified, you must also specify the `project` field to provide a default object location.\"\n",
    "            )\n",
    "        return v\n",
    "\n",
    "    @field_validator(\"quota_project\")\n",
    "    def validate_quota_project(\n",
    "        cls,\n",
    "        v: t.Optional[str],\n",
    "        info: ValidationInfo,\n",
    "    ) -> t.Optional[str]:\n",
    "        if v and not info.data.get(\"project\"):\n",
    "            raise ConfigError(\n",
    "                \"If the `quota_project` field is specified, you must also specify the `project` field to provide a default object location.\"\n",
    "            )\n",
    "        return v\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        return set()\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        return engine_adapter.BigQueryEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _static_connection_kwargs(self) -> t.Dict[str, t.Any]:\n",
    "        \"\"\"The static connection kwargs for this connection\"\"\"\n",
    "        import google.auth\n",
    "        from google.auth import impersonated_credentials\n",
    "        from google.api_core import client_info, client_options\n",
    "        from google.oauth2 import credentials, service_account\n",
    "\n",
    "        if self.method == BigQueryConnectionMethod.OAUTH:\n",
    "            creds, _ = google.auth.default(scopes=self.scopes)\n",
    "        elif self.method == BigQueryConnectionMethod.SERVICE_ACCOUNT:\n",
    "            creds = service_account.Credentials.from_service_account_file(\n",
    "                self.keyfile, scopes=self.scopes\n",
    "            )\n",
    "        elif self.method == BigQueryConnectionMethod.SERVICE_ACCOUNT_JSON:\n",
    "            creds = service_account.Credentials.from_service_account_info(\n",
    "                self.keyfile_json, scopes=self.scopes\n",
    "            )\n",
    "        elif self.method == BigQueryConnectionMethod.OAUTH_SECRETS:\n",
    "            creds = credentials.Credentials(\n",
    "                token=self.token,\n",
    "                refresh_token=self.refresh_token,\n",
    "                client_id=self.client_id,\n",
    "                client_secret=self.client_secret,\n",
    "                token_uri=self.token_uri,\n",
    "                scopes=self.scopes,\n",
    "            )\n",
    "        else:\n",
    "            raise ConfigError(\"Invalid BigQuery Connection Method\")\n",
    "\n",
    "        if self.impersonated_service_account:\n",
    "            creds = impersonated_credentials.Credentials(\n",
    "                source_credentials=creds,\n",
    "                target_principal=self.impersonated_service_account,\n",
    "                target_scopes=self.scopes,\n",
    "            )\n",
    "\n",
    "        options = client_options.ClientOptions(quota_project_id=self.quota_project)\n",
    "        project = self.execution_project or self.project or None\n",
    "\n",
    "        client = google.cloud.bigquery.Client(\n",
    "            project=project and exp.parse_identifier(project, dialect=\"bigquery\").name,\n",
    "            credentials=creds,\n",
    "            location=self.location,\n",
    "            client_info=client_info.ClientInfo(user_agent=\"sqlmesh\"),\n",
    "            client_options=options,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"client\": client,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _extra_engine_config(self) -> t.Dict[str, t.Any]:\n",
    "        return {\n",
    "            k: v\n",
    "            for k, v in self.dict().items()\n",
    "            if k\n",
    "            in {\n",
    "                \"job_creation_timeout_seconds\",\n",
    "                \"job_execution_timeout_seconds\",\n",
    "                \"job_retries\",\n",
    "                \"job_retry_deadline_seconds\",\n",
    "                \"priority\",\n",
    "                \"maximum_bytes_billed\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        from google.cloud.bigquery.dbapi import connect\n",
    "\n",
    "        return connect\n",
    "\n",
    "    def get_catalog(self) -> t.Optional[str]:\n",
    "        return self.project\n",
    "\n",
    "\n",
    "class GCPPostgresConnectionConfig(ConnectionConfig):\n",
    "    \"\"\"\n",
    "    Postgres Connection Configuration for GCP.\n",
    "\n",
    "    Args:\n",
    "        instance_connection_string: Connection name for the postgres instance.\n",
    "        user: Postgres or IAM user's name\n",
    "        password: The postgres user's password. Only needed when the user is a postgres user.\n",
    "        enable_iam_auth: Set to True when user is an IAM user.\n",
    "        db: Name of the db to connect to.\n",
    "        keyfile: string path to json service account credentials file\n",
    "        keyfile_json: dict service account credentials info\n",
    "        pre_ping: Whether or not to pre-ping the connection before starting a new transaction to ensure it is still alive.\n",
    "    \"\"\"\n",
    "\n",
    "    instance_connection_string: str\n",
    "    user: str\n",
    "    password: t.Optional[str] = None\n",
    "    enable_iam_auth: t.Optional[bool] = None\n",
    "    db: str\n",
    "    ip_type: t.Union[t.Literal[\"public\"], t.Literal[\"private\"], t.Literal[\"psc\"]] = \"public\"\n",
    "    # Keyfile Auth\n",
    "    keyfile: t.Optional[str] = None\n",
    "    keyfile_json: t.Optional[t.Dict[str, t.Any]] = None\n",
    "    timeout: t.Optional[int] = None\n",
    "    scopes: t.Tuple[str, ...] = (\"https://www.googleapis.com/auth/sqlservice.admin\",)\n",
    "    driver: str = \"pg8000\"\n",
    "    type_: t.Literal[\"gcp_postgres\"] = Field(alias=\"type\", default=\"gcp_postgres\")\n",
    "    concurrent_tasks: int = 4\n",
    "    register_comments: bool = True\n",
    "    pre_ping: bool = True\n",
    "\n",
    "    _engine_import_validator = _get_engine_import_validator(\n",
    "        \"google.cloud.sql\", \"gcp_postgres\", \"gcppostgres\"\n",
    "    )\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    def _validate_auth_method(cls, data: t.Any) -> t.Any:\n",
    "        if not isinstance(data, dict):\n",
    "            return data\n",
    "\n",
    "        password = data.get(\"password\")\n",
    "        enable_iam_auth = data.get(\"enable_iam_auth\")\n",
    "\n",
    "        if password and enable_iam_auth:\n",
    "            raise ConfigError(\n",
    "                \"Invalid GCP Postgres connection configuration - both password and\"\n",
    "                \" enable_iam_auth set. Use password when connecting to a postgres\"\n",
    "                \" user and enable_iam_auth 'True' when connecting to an IAM user.\"\n",
    "            )\n",
    "        if not password and not enable_iam_auth:\n",
    "            raise ConfigError(\n",
    "                \"GCP Postgres connection configuration requires either password set\"\n",
    "                \" for a postgres user account or enable_iam_auth set to 'True'\"\n",
    "                \" for an IAM user account.\"\n",
    "            )\n",
    "\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        return {\n",
    "            \"instance_connection_string\",\n",
    "            \"driver\",\n",
    "            \"user\",\n",
    "            \"password\",\n",
    "            \"db\",\n",
    "            \"enable_iam_auth\",\n",
    "            \"timeout\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        return engine_adapter.PostgresEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        from google.cloud.sql.connector import Connector\n",
    "        from google.oauth2 import service_account\n",
    "\n",
    "        creds = None\n",
    "        if self.keyfile:\n",
    "            creds = service_account.Credentials.from_service_account_file(\n",
    "                self.keyfile, scopes=self.scopes\n",
    "            )\n",
    "        elif self.keyfile_json:\n",
    "            creds = service_account.Credentials.from_service_account_info(\n",
    "                self.keyfile_json, scopes=self.scopes\n",
    "            )\n",
    "\n",
    "        kwargs = {\n",
    "            \"credentials\": creds,\n",
    "            \"ip_type\": self.ip_type,\n",
    "        }\n",
    "\n",
    "        if self.timeout:\n",
    "            kwargs[\"timeout\"] = self.timeout\n",
    "\n",
    "        return Connector(**kwargs).connect  # type: ignore\n",
    "\n",
    "\n",
    "class RedshiftConnectionConfig(ConnectionConfig):\n",
    "    \"\"\"\n",
    "    Redshift Connection Configuration.\n",
    "\n",
    "    Arg Source: https://github.com/aws/amazon-redshift-python-driver/blob/master/redshift_connector/__init__.py#L146\n",
    "    Note: A subset of properties were selected. Please open an issue/PR if you want to see more supported.\n",
    "\n",
    "    Args:\n",
    "        user: The username to use for authentication with the Amazon Redshift cluster.\n",
    "        password: The password to use for authentication with the Amazon Redshift cluster.\n",
    "        database: The name of the database instance to connect to.\n",
    "        host: The hostname of the Amazon Redshift cluster.\n",
    "        port: The port number of the Amazon Redshift cluster. Default value is 5439.\n",
    "        source_address: No description provided\n",
    "        unix_sock: No description provided\n",
    "        ssl: Is SSL enabled. Default value is ``True``. SSL must be enabled when authenticating using IAM.\n",
    "        sslmode: The security of the connection to the Amazon Redshift cluster. 'verify-ca' and 'verify-full' are supported.\n",
    "        timeout: The number of seconds before the connection to the server will timeout. By default there is no timeout.\n",
    "        tcp_keepalive: Is `TCP keepalive <https://en.wikipedia.org/wiki/Keepalive#TCP_keepalive>`_ used. The default value is ``True``.\n",
    "        application_name: Sets the application name. The default value is None.\n",
    "        preferred_role: The IAM role preferred for the current connection.\n",
    "        principal_arn: The ARN of the IAM entity (user or role) for which you are generating a policy.\n",
    "        credentials_provider: The class name of the IdP that will be used for authenticating with the Amazon Redshift cluster.\n",
    "        region: The AWS region where the Amazon Redshift cluster is located.\n",
    "        cluster_identifier: The cluster identifier of the Amazon Redshift cluster.\n",
    "        iam: If IAM authentication is enabled. Default value is False. IAM must be True when authenticating using an IdP.\n",
    "        is_serverless: Redshift end-point is serverless or provisional. Default value false.\n",
    "        serverless_acct_id: The account ID of the serverless. Default value None\n",
    "        serverless_work_group: The name of work group for serverless end point. Default value None.\n",
    "        pre_ping: Whether or not to pre-ping the connection before starting a new transaction to ensure it is still alive.\n",
    "        enable_merge: Whether to use the Redshift merge operation instead of the SQLMesh logical merge.\n",
    "    \"\"\"\n",
    "\n",
    "    user: t.Optional[str] = None\n",
    "    password: t.Optional[str] = None\n",
    "    database: t.Optional[str] = None\n",
    "    host: t.Optional[str] = None\n",
    "    port: t.Optional[int] = None\n",
    "    source_address: t.Optional[str] = None\n",
    "    unix_sock: t.Optional[str] = None\n",
    "    ssl: t.Optional[bool] = None\n",
    "    sslmode: t.Optional[str] = None\n",
    "    timeout: t.Optional[int] = None\n",
    "    tcp_keepalive: t.Optional[bool] = None\n",
    "    application_name: t.Optional[str] = None\n",
    "    preferred_role: t.Optional[str] = None\n",
    "    principal_arn: t.Optional[str] = None\n",
    "    credentials_provider: t.Optional[str] = None\n",
    "    region: t.Optional[str] = None\n",
    "    cluster_identifier: t.Optional[str] = None\n",
    "    iam: t.Optional[bool] = None\n",
    "    is_serverless: t.Optional[bool] = None\n",
    "    serverless_acct_id: t.Optional[str] = None\n",
    "    serverless_work_group: t.Optional[str] = None\n",
    "    enable_merge: t.Optional[bool] = None\n",
    "\n",
    "    concurrent_tasks: int = 4\n",
    "    register_comments: bool = True\n",
    "    pre_ping: bool = False\n",
    "\n",
    "    type_: t.Literal[\"redshift\"] = Field(alias=\"type\", default=\"redshift\")\n",
    "\n",
    "    _engine_import_validator = _get_engine_import_validator(\"redshift_connector\", \"redshift\")\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        return {\n",
    "            \"user\",\n",
    "            \"password\",\n",
    "            \"database\",\n",
    "            \"host\",\n",
    "            \"port\",\n",
    "            \"source_address\",\n",
    "            \"unix_sock\",\n",
    "            \"ssl\",\n",
    "            \"sslmode\",\n",
    "            \"timeout\",\n",
    "            \"tcp_keepalive\",\n",
    "            \"application_name\",\n",
    "            \"preferred_role\",\n",
    "            \"principal_arn\",\n",
    "            \"credentials_provider\",\n",
    "            \"region\",\n",
    "            \"cluster_identifier\",\n",
    "            \"iam\",\n",
    "            \"is_serverless\",\n",
    "            \"serverless_acct_id\",\n",
    "            \"serverless_work_group\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        return engine_adapter.RedshiftEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        from redshift_connector import connect\n",
    "\n",
    "        return connect\n",
    "\n",
    "    @property\n",
    "    def _extra_engine_config(self) -> t.Dict[str, t.Any]:\n",
    "        return {\"enable_merge\": self.enable_merge}\n",
    "\n",
    "\n",
    "class PostgresConnectionConfig(ConnectionConfig):\n",
    "    host: str\n",
    "    user: str\n",
    "    password: str\n",
    "    port: int\n",
    "    database: str\n",
    "    keepalives_idle: t.Optional[int] = None\n",
    "    connect_timeout: int = 10\n",
    "    role: t.Optional[str] = None\n",
    "    sslmode: t.Optional[str] = None\n",
    "    application_name: t.Optional[str] = None\n",
    "\n",
    "    concurrent_tasks: int = 4\n",
    "    register_comments: bool = True\n",
    "    pre_ping: bool = True\n",
    "\n",
    "    type_: t.Literal[\"postgres\"] = Field(alias=\"type\", default=\"postgres\")\n",
    "\n",
    "    _engine_import_validator = _get_engine_import_validator(\"psycopg2\", \"postgres\")\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        return {\n",
    "            \"host\",\n",
    "            \"user\",\n",
    "            \"password\",\n",
    "            \"port\",\n",
    "            \"database\",\n",
    "            \"keepalives_idle\",\n",
    "            \"connect_timeout\",\n",
    "            \"sslmode\",\n",
    "            \"application_name\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        return engine_adapter.PostgresEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        from psycopg2 import connect\n",
    "\n",
    "        return connect\n",
    "\n",
    "    @property\n",
    "    def _cursor_init(self) -> t.Optional[t.Callable[[t.Any], None]]:\n",
    "        if not self.role:\n",
    "            return None\n",
    "\n",
    "        def init(cursor: t.Any) -> None:\n",
    "            cursor.execute(f\"SET ROLE {self.role}\")\n",
    "\n",
    "        return init\n",
    "\n",
    "\n",
    "class MySQLConnectionConfig(ConnectionConfig):\n",
    "    host: str\n",
    "    user: str\n",
    "    password: str\n",
    "    port: t.Optional[int] = None\n",
    "    database: t.Optional[str] = None\n",
    "    charset: t.Optional[str] = None\n",
    "    collation: t.Optional[str] = None\n",
    "    ssl_disabled: t.Optional[bool] = None\n",
    "\n",
    "    concurrent_tasks: int = 4\n",
    "    register_comments: bool = True\n",
    "    pre_ping: bool = True\n",
    "\n",
    "    type_: t.Literal[\"mysql\"] = Field(alias=\"type\", default=\"mysql\")\n",
    "\n",
    "    _engine_import_validator = _get_engine_import_validator(\"pymysql\", \"mysql\")\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        connection_keys = {\n",
    "            \"host\",\n",
    "            \"user\",\n",
    "            \"password\",\n",
    "        }\n",
    "        if self.port is not None:\n",
    "            connection_keys.add(\"port\")\n",
    "        if self.database is not None:\n",
    "            connection_keys.add(\"database\")\n",
    "        if self.charset is not None:\n",
    "            connection_keys.add(\"charset\")\n",
    "        if self.collation is not None:\n",
    "            connection_keys.add(\"collation\")\n",
    "        if self.ssl_disabled is not None:\n",
    "            connection_keys.add(\"ssl_disabled\")\n",
    "        return connection_keys\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        return engine_adapter.MySQLEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        from pymysql import connect\n",
    "\n",
    "        return connect\n",
    "\n",
    "\n",
    "class MSSQLConnectionConfig(ConnectionConfig):\n",
    "    host: str\n",
    "    user: t.Optional[str] = None\n",
    "    password: t.Optional[str] = None\n",
    "    database: t.Optional[str] = \"\"\n",
    "    timeout: t.Optional[int] = 0\n",
    "    login_timeout: t.Optional[int] = 60\n",
    "    charset: t.Optional[str] = \"UTF-8\"\n",
    "    appname: t.Optional[str] = None\n",
    "    port: t.Optional[int] = 1433\n",
    "    conn_properties: t.Optional[t.Union[t.List[str], str]] = None\n",
    "    autocommit: t.Optional[bool] = False\n",
    "    tds_version: t.Optional[str] = None\n",
    "\n",
    "    # Driver options\n",
    "    driver: t.Literal[\"pymssql\", \"pyodbc\"] = \"pymssql\"\n",
    "    # PyODBC specific options\n",
    "    driver_name: t.Optional[str] = None  # e.g. \"ODBC Driver 18 for SQL Server\"\n",
    "    trust_server_certificate: t.Optional[bool] = None\n",
    "    encrypt: t.Optional[bool] = None\n",
    "    # Dictionary of arbitrary ODBC connection properties\n",
    "    # See: https://learn.microsoft.com/en-us/sql/connect/odbc/dsn-connection-string-attribute\n",
    "    odbc_properties: t.Optional[t.Dict[str, t.Any]] = None\n",
    "\n",
    "    concurrent_tasks: int = 4\n",
    "    register_comments: bool = True\n",
    "    pre_ping: bool = True\n",
    "\n",
    "    type_: t.Literal[\"mssql\"] = Field(alias=\"type\", default=\"mssql\")\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    @classmethod\n",
    "    def _mssql_engine_import_validator(cls, data: t.Any) -> t.Any:\n",
    "        if not isinstance(data, dict):\n",
    "            return data\n",
    "\n",
    "        driver = data.get(\"driver\", \"pymssql\")\n",
    "\n",
    "        # Define the mapping of driver to import module and extra name\n",
    "        driver_configs = {\"pymssql\": (\"pymssql\", \"mssql\"), \"pyodbc\": (\"pyodbc\", \"mssql-odbc\")}\n",
    "\n",
    "        if driver not in driver_configs:\n",
    "            raise ValueError(f\"Unsupported driver: {driver}\")\n",
    "\n",
    "        import_module, extra_name = driver_configs[driver]\n",
    "\n",
    "        # Use _get_engine_import_validator with decorate=False to get the raw validation function\n",
    "        # This avoids the __wrapped__ issue in Python 3.9\n",
    "        validator_func = _get_engine_import_validator(\n",
    "            import_module, driver, extra_name, decorate=False\n",
    "        )\n",
    "\n",
    "        # Call the raw validation function directly\n",
    "        return validator_func(cls, data)\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        base_keys = {\n",
    "            \"host\",\n",
    "            \"user\",\n",
    "            \"password\",\n",
    "            \"database\",\n",
    "            \"timeout\",\n",
    "            \"login_timeout\",\n",
    "            \"charset\",\n",
    "            \"appname\",\n",
    "            \"port\",\n",
    "            \"conn_properties\",\n",
    "            \"autocommit\",\n",
    "            \"tds_version\",\n",
    "        }\n",
    "\n",
    "        if self.driver == \"pyodbc\":\n",
    "            base_keys.update(\n",
    "                {\n",
    "                    \"driver_name\",\n",
    "                    \"trust_server_certificate\",\n",
    "                    \"encrypt\",\n",
    "                    \"odbc_properties\",\n",
    "                }\n",
    "            )\n",
    "            # Remove pymssql-specific parameters\n",
    "            base_keys.discard(\"tds_version\")\n",
    "            base_keys.discard(\"conn_properties\")\n",
    "\n",
    "        return base_keys\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        return engine_adapter.MSSQLEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        if self.driver == \"pymssql\":\n",
    "            import pymssql\n",
    "\n",
    "            return pymssql.connect\n",
    "\n",
    "        import pyodbc\n",
    "\n",
    "        def connect(**kwargs: t.Any) -> t.Callable:\n",
    "            # Extract parameters for connection string\n",
    "            host = kwargs.pop(\"host\")\n",
    "            port = kwargs.pop(\"port\", 1433)\n",
    "            database = kwargs.pop(\"database\", \"\")\n",
    "            user = kwargs.pop(\"user\", None)\n",
    "            password = kwargs.pop(\"password\", None)\n",
    "            driver_name = kwargs.pop(\"driver_name\", \"ODBC Driver 18 for SQL Server\")\n",
    "            trust_server_certificate = kwargs.pop(\"trust_server_certificate\", False)\n",
    "            encrypt = kwargs.pop(\"encrypt\", True)\n",
    "            login_timeout = kwargs.pop(\"login_timeout\", 60)\n",
    "\n",
    "            # Build connection string\n",
    "            conn_str_parts = [\n",
    "                f\"DRIVER={{{driver_name}}}\",\n",
    "                f\"SERVER={host},{port}\",\n",
    "            ]\n",
    "\n",
    "            if database:\n",
    "                conn_str_parts.append(f\"DATABASE={database}\")\n",
    "\n",
    "            # Add security options\n",
    "            conn_str_parts.append(f\"Encrypt={'YES' if encrypt else 'NO'}\")\n",
    "            if trust_server_certificate:\n",
    "                conn_str_parts.append(\"TrustServerCertificate=YES\")\n",
    "\n",
    "            conn_str_parts.append(f\"Connection Timeout={login_timeout}\")\n",
    "\n",
    "            # Standard SQL Server authentication\n",
    "            if user:\n",
    "                conn_str_parts.append(f\"UID={user}\")\n",
    "            if password:\n",
    "                conn_str_parts.append(f\"PWD={password}\")\n",
    "\n",
    "            # Add any additional ODBC properties from the odbc_properties dictionary\n",
    "            if self.odbc_properties:\n",
    "                for key, value in self.odbc_properties.items():\n",
    "                    # Skip properties that we've already set above\n",
    "                    if key.lower() in (\n",
    "                        \"driver\",\n",
    "                        \"server\",\n",
    "                        \"database\",\n",
    "                        \"uid\",\n",
    "                        \"pwd\",\n",
    "                        \"encrypt\",\n",
    "                        \"trustservercertificate\",\n",
    "                        \"connection timeout\",\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    # Handle boolean values properly\n",
    "                    if isinstance(value, bool):\n",
    "                        conn_str_parts.append(f\"{key}={'YES' if value else 'NO'}\")\n",
    "                    else:\n",
    "                        conn_str_parts.append(f\"{key}={value}\")\n",
    "\n",
    "            # Create the connection string\n",
    "            conn_str = \";\".join(conn_str_parts)\n",
    "\n",
    "            return pyodbc.connect(conn_str, autocommit=kwargs.get(\"autocommit\", False))\n",
    "\n",
    "        return connect\n",
    "\n",
    "    @property\n",
    "    def _extra_engine_config(self) -> t.Dict[str, t.Any]:\n",
    "        return {\"catalog_support\": CatalogSupport.REQUIRES_SET_CATALOG}\n",
    "\n",
    "\n",
    "class AzureSQLConnectionConfig(MSSQLConnectionConfig):\n",
    "    type_: t.Literal[\"azuresql\"] = Field(alias=\"type\", default=\"azuresql\")  # type: ignore\n",
    "\n",
    "    @property\n",
    "    def _extra_engine_config(self) -> t.Dict[str, t.Any]:\n",
    "        return {\"catalog_support\": CatalogSupport.SINGLE_CATALOG_ONLY}\n",
    "\n",
    "\n",
    "class SparkConnectionConfig(ConnectionConfig):\n",
    "    \"\"\"\n",
    "    Vanilla Spark Connection Configuration. Use `DatabricksConnectionConfig` for Databricks.\n",
    "    \"\"\"\n",
    "\n",
    "    config_dir: t.Optional[str] = None\n",
    "    catalog: t.Optional[str] = None\n",
    "    config: t.Dict[str, t.Any] = {}\n",
    "\n",
    "    concurrent_tasks: int = 4\n",
    "    register_comments: bool = True\n",
    "    pre_ping: t.Literal[False] = False\n",
    "\n",
    "    type_: t.Literal[\"spark\"] = Field(alias=\"type\", default=\"spark\")\n",
    "\n",
    "    _engine_import_validator = _get_engine_import_validator(\"pyspark\", \"spark\")\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        return {\n",
    "            \"catalog\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        return engine_adapter.SparkEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        from sqlmesh.engines.spark.db_api.spark_session import connection\n",
    "\n",
    "        return connection\n",
    "\n",
    "    @property\n",
    "    def _static_connection_kwargs(self) -> t.Dict[str, t.Any]:\n",
    "        from pyspark.conf import SparkConf\n",
    "        from pyspark.sql import SparkSession\n",
    "\n",
    "        spark_config = SparkConf()\n",
    "        if self.config:\n",
    "            for k, v in self.config.items():\n",
    "                spark_config.set(k, v)\n",
    "\n",
    "        if self.config_dir:\n",
    "            os.environ[\"SPARK_CONF_DIR\"] = self.config_dir\n",
    "        return {\n",
    "            \"spark\": SparkSession.builder.config(conf=spark_config)\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate(),\n",
    "        }\n",
    "\n",
    "\n",
    "class TrinoAuthenticationMethod(str, Enum):\n",
    "    NO_AUTH = \"no-auth\"\n",
    "    BASIC = \"basic\"\n",
    "    LDAP = \"ldap\"\n",
    "    KERBEROS = \"kerberos\"\n",
    "    JWT = \"jwt\"\n",
    "    CERTIFICATE = \"certificate\"\n",
    "    OAUTH = \"oauth\"\n",
    "\n",
    "    @property\n",
    "    def is_no_auth(self) -> bool:\n",
    "        return self == self.NO_AUTH\n",
    "\n",
    "    @property\n",
    "    def is_basic(self) -> bool:\n",
    "        return self == self.BASIC\n",
    "\n",
    "    @property\n",
    "    def is_ldap(self) -> bool:\n",
    "        return self == self.LDAP\n",
    "\n",
    "    @property\n",
    "    def is_kerberos(self) -> bool:\n",
    "        return self == self.KERBEROS\n",
    "\n",
    "    @property\n",
    "    def is_jwt(self) -> bool:\n",
    "        return self == self.JWT\n",
    "\n",
    "    @property\n",
    "    def is_certificate(self) -> bool:\n",
    "        return self == self.CERTIFICATE\n",
    "\n",
    "    @property\n",
    "    def is_oauth(self) -> bool:\n",
    "        return self == self.OAUTH\n",
    "\n",
    "\n",
    "class TrinoConnectionConfig(ConnectionConfig):\n",
    "    method: TrinoAuthenticationMethod = TrinoAuthenticationMethod.NO_AUTH\n",
    "    host: str\n",
    "    user: str\n",
    "    catalog: str\n",
    "    port: t.Optional[int] = None\n",
    "    http_scheme: t.Literal[\"http\", \"https\"] = \"https\"\n",
    "    # General Optional\n",
    "    roles: t.Optional[t.Dict[str, str]] = None\n",
    "    http_headers: t.Optional[t.Dict[str, str]] = None\n",
    "    session_properties: t.Optional[t.Dict[str, str]] = None\n",
    "    retries: int = 3\n",
    "    timezone: t.Optional[str] = None\n",
    "    # Basic/LDAP\n",
    "    password: t.Optional[str] = None\n",
    "    verify: t.Optional[bool] = None  # disable SSL verification (ignored if `cert` is provided)\n",
    "    # LDAP\n",
    "    impersonation_user: t.Optional[str] = None\n",
    "    # Kerberos\n",
    "    keytab: t.Optional[str] = None\n",
    "    krb5_config: t.Optional[str] = None\n",
    "    principal: t.Optional[str] = None\n",
    "    service_name: str = \"trino\"\n",
    "    hostname_override: t.Optional[str] = None\n",
    "    mutual_authentication: bool = False\n",
    "    force_preemptive: bool = False\n",
    "    sanitize_mutual_error_response: bool = True\n",
    "    delegate: bool = False\n",
    "    # JWT\n",
    "    jwt_token: t.Optional[str] = None\n",
    "    # Certificate\n",
    "    client_certificate: t.Optional[str] = None\n",
    "    client_private_key: t.Optional[str] = None\n",
    "    cert: t.Optional[str] = None\n",
    "\n",
    "    # SQLMesh options\n",
    "    schema_location_mapping: t.Optional[dict[re.Pattern, str]] = None\n",
    "    concurrent_tasks: int = 4\n",
    "    register_comments: bool = True\n",
    "    pre_ping: t.Literal[False] = False\n",
    "\n",
    "    type_: t.Literal[\"trino\"] = Field(alias=\"type\", default=\"trino\")\n",
    "\n",
    "    _engine_import_validator = _get_engine_import_validator(\"trino\", \"trino\")\n",
    "\n",
    "    @field_validator(\"schema_location_mapping\", mode=\"before\")\n",
    "    @classmethod\n",
    "    def _validate_regex_keys(\n",
    "        cls, value: t.Dict[str | re.Pattern, str]\n",
    "    ) -> t.Dict[re.Pattern, t.Any]:\n",
    "        compiled = compile_regex_mapping(value)\n",
    "        for replacement in compiled.values():\n",
    "            if \"@{schema_name}\" not in replacement:\n",
    "                raise ConfigError(\n",
    "                    \"schema_location_mapping needs to include the '@{schema_name}' placeholder in the value so SQLMesh knows where to substitute the schema name\"\n",
    "                )\n",
    "        return compiled\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def _root_validator(self) -> Self:\n",
    "        port = self.port\n",
    "        if self.http_scheme == \"http\" and not self.method.is_no_auth and not self.method.is_basic:\n",
    "            raise ConfigError(\"HTTP scheme can only be used with no-auth or basic method\")\n",
    "\n",
    "        if port is None:\n",
    "            self.port = 80 if self.http_scheme == \"http\" else 443\n",
    "\n",
    "        if (self.method.is_ldap or self.method.is_basic) and (not self.password or not self.user):\n",
    "            raise ConfigError(\n",
    "                f\"Username and Password must be provided if using {self.method.value} authentication\"\n",
    "            )\n",
    "\n",
    "        if self.method.is_kerberos and (\n",
    "            not self.principal or not self.keytab or not self.krb5_config\n",
    "        ):\n",
    "            raise ConfigError(\n",
    "                \"Kerberos requires the following fields: principal, keytab, and krb5_config\"\n",
    "            )\n",
    "\n",
    "        if self.method.is_jwt and not self.jwt_token:\n",
    "            raise ConfigError(\"JWT requires `jwt_token` to be set\")\n",
    "\n",
    "        if self.method.is_certificate and (\n",
    "            not self.cert or not self.client_certificate or not self.client_private_key\n",
    "        ):\n",
    "            raise ConfigError(\n",
    "                \"Certificate requires the following fields: cert, client_certificate, and client_private_key\"\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        kwargs = {\n",
    "            \"host\",\n",
    "            \"port\",\n",
    "            \"catalog\",\n",
    "            \"roles\",\n",
    "            \"http_scheme\",\n",
    "            \"http_headers\",\n",
    "            \"session_properties\",\n",
    "            \"timezone\",\n",
    "        }\n",
    "        return kwargs\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        return engine_adapter.TrinoEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        from trino.dbapi import connect\n",
    "\n",
    "        return connect\n",
    "\n",
    "    @property\n",
    "    def _static_connection_kwargs(self) -> t.Dict[str, t.Any]:\n",
    "        from trino.auth import (\n",
    "            BasicAuthentication,\n",
    "            CertificateAuthentication,\n",
    "            JWTAuthentication,\n",
    "            KerberosAuthentication,\n",
    "            OAuth2Authentication,\n",
    "        )\n",
    "\n",
    "        if self.method.is_basic or self.method.is_ldap:\n",
    "            auth = BasicAuthentication(self.user, self.password)\n",
    "        elif self.method.is_kerberos:\n",
    "            if self.keytab:\n",
    "                os.environ[\"KRB5_CLIENT_KTNAME\"] = self.keytab\n",
    "            auth = KerberosAuthentication(\n",
    "                config=self.krb5_config,\n",
    "                service_name=self.service_name,\n",
    "                principal=self.principal,\n",
    "                mutual_authentication=self.mutual_authentication,\n",
    "                ca_bundle=self.cert,\n",
    "                force_preemptive=self.force_preemptive,\n",
    "                hostname_override=self.hostname_override,\n",
    "                sanitize_mutual_error_response=self.sanitize_mutual_error_response,\n",
    "                delegate=self.delegate,\n",
    "            )\n",
    "        elif self.method.is_oauth:\n",
    "            auth = OAuth2Authentication()\n",
    "        elif self.method.is_jwt:\n",
    "            auth = JWTAuthentication(self.jwt_token)\n",
    "        elif self.method.is_certificate:\n",
    "            auth = CertificateAuthentication(self.client_certificate, self.client_private_key)\n",
    "        else:\n",
    "            auth = None\n",
    "\n",
    "        return {\n",
    "            \"auth\": auth,\n",
    "            \"user\": self.impersonation_user or self.user,\n",
    "            \"max_attempts\": self.retries,\n",
    "            \"verify\": self.cert if self.cert is not None else self.verify,\n",
    "            \"source\": \"sqlmesh\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _extra_engine_config(self) -> t.Dict[str, t.Any]:\n",
    "        return {\"schema_location_mapping\": self.schema_location_mapping}\n",
    "\n",
    "\n",
    "class ClickhouseConnectionConfig(ConnectionConfig):\n",
    "    \"\"\"\n",
    "    Clickhouse Connection Configuration.\n",
    "\n",
    "    Property reference: https://clickhouse.com/docs/en/integrations/python#client-initialization\n",
    "    \"\"\"\n",
    "\n",
    "    host: str\n",
    "    username: str\n",
    "    password: t.Optional[str] = None\n",
    "    port: t.Optional[int] = None\n",
    "    cluster: t.Optional[str] = None\n",
    "    connect_timeout: int = 10\n",
    "    send_receive_timeout: int = 300\n",
    "    query_limit: int = 0\n",
    "    use_compression: bool = True\n",
    "    compression_method: t.Optional[str] = None\n",
    "    connection_settings: t.Optional[t.Dict[str, t.Any]] = None\n",
    "    http_proxy: t.Optional[str] = None\n",
    "    # HTTPS/TLS settings\n",
    "    verify: bool = True\n",
    "    ca_cert: t.Optional[str] = None\n",
    "    client_cert: t.Optional[str] = None\n",
    "    client_cert_key: t.Optional[str] = None\n",
    "    https_proxy: t.Optional[str] = None\n",
    "    server_host_name: t.Optional[str] = None\n",
    "    tls_mode: t.Optional[str] = None\n",
    "\n",
    "    concurrent_tasks: int = 1\n",
    "    register_comments: bool = True\n",
    "    pre_ping: bool = False\n",
    "\n",
    "    # This object expects options from urllib3 and also from clickhouse-connect\n",
    "    # See:\n",
    "    # * https://urllib3.readthedocs.io/en/stable/advanced-usage.html\n",
    "    # * https://clickhouse.com/docs/en/integrations/python#customizing-the-http-connection-pool\n",
    "    connection_pool_options: t.Optional[t.Dict[str, t.Any]] = None\n",
    "\n",
    "    type_: t.Literal[\"clickhouse\"] = Field(alias=\"type\", default=\"clickhouse\")\n",
    "\n",
    "    _engine_import_validator = _get_engine_import_validator(\"clickhouse_connect\", \"clickhouse\")\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        kwargs = {\n",
    "            \"host\",\n",
    "            \"username\",\n",
    "            \"port\",\n",
    "            \"password\",\n",
    "            \"connect_timeout\",\n",
    "            \"send_receive_timeout\",\n",
    "            \"query_limit\",\n",
    "            \"http_proxy\",\n",
    "            \"verify\",\n",
    "            \"ca_cert\",\n",
    "            \"client_cert\",\n",
    "            \"client_cert_key\",\n",
    "            \"https_proxy\",\n",
    "            \"server_host_name\",\n",
    "            \"tls_mode\",\n",
    "        }\n",
    "        return kwargs\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        return engine_adapter.ClickhouseEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        from clickhouse_connect.dbapi import connect  # type: ignore\n",
    "        from clickhouse_connect.driver import httputil  # type: ignore\n",
    "        from functools import partial\n",
    "\n",
    "        pool_manager_options: t.Dict[str, t.Any] = dict(\n",
    "            # Match the maxsize to the number of concurrent tasks\n",
    "            maxsize=self.concurrent_tasks,\n",
    "            # Block if there are no free connections\n",
    "            block=True,\n",
    "            verify=self.verify,\n",
    "            ca_cert=self.ca_cert,\n",
    "            client_cert=self.client_cert,\n",
    "            client_cert_key=self.client_cert_key,\n",
    "            https_proxy=self.https_proxy,\n",
    "        )\n",
    "        # this doesn't happen automatically because we always supply our own pool manager to the connection\n",
    "        # https://github.com/ClickHouse/clickhouse-connect/blob/3a7f4b04cad29c7c2536661b831fb744248e2ec0/clickhouse_connect/driver/httpclient.py#L109\n",
    "        if self.server_host_name:\n",
    "            pool_manager_options[\"server_hostname\"] = self.server_host_name\n",
    "            if self.verify:\n",
    "                pool_manager_options[\"assert_hostname\"] = self.server_host_name\n",
    "        if self.connection_pool_options:\n",
    "            pool_manager_options.update(self.connection_pool_options)\n",
    "        pool_mgr = httputil.get_pool_manager(**pool_manager_options)\n",
    "\n",
    "        return partial(connect, pool_mgr=pool_mgr)\n",
    "\n",
    "    @property\n",
    "    def cloud_mode(self) -> bool:\n",
    "        return \"clickhouse.cloud\" in self.host\n",
    "\n",
    "    @property\n",
    "    def _extra_engine_config(self) -> t.Dict[str, t.Any]:\n",
    "        return {\"cluster\": self.cluster, \"cloud_mode\": self.cloud_mode}\n",
    "\n",
    "    @property\n",
    "    def _static_connection_kwargs(self) -> t.Dict[str, t.Any]:\n",
    "        from sqlmesh import __version__\n",
    "\n",
    "        # False = no compression\n",
    "        # True = Clickhouse default compression method\n",
    "        # string = specific compression method\n",
    "        compress: bool | str = self.use_compression\n",
    "        if compress and self.compression_method:\n",
    "            compress = self.compression_method\n",
    "\n",
    "        # Clickhouse system settings passed to connection\n",
    "        # https://clickhouse.com/docs/en/operations/settings/settings\n",
    "        # - below are set to align with dbt-clickhouse\n",
    "        # - https://github.com/ClickHouse/dbt-clickhouse/blob/44d26308ea6a3c8ead25c280164aa88191f05f47/dbt/adapters/clickhouse/dbclient.py#L77\n",
    "        settings = self.connection_settings or {}\n",
    "        #  mutations_sync = 2: \"The query waits for all mutations [ALTER statements] to complete on all replicas (if they exist)\"\n",
    "        settings[\"mutations_sync\"] = \"2\"\n",
    "        #  insert_distributed_sync = 1: \"INSERT operation succeeds only after all the data is saved on all shards\"\n",
    "        settings[\"insert_distributed_sync\"] = \"1\"\n",
    "        if self.cluster or self.cloud_mode:\n",
    "            # database_replicated_enforce_synchronous_settings = 1:\n",
    "            #   - \"Enforces synchronous waiting for some queries\"\n",
    "            #   - https://github.com/ClickHouse/ClickHouse/blob/ccaa8d03a9351efc16625340268b9caffa8a22ba/src/Core/Settings.h#L709\n",
    "            settings[\"database_replicated_enforce_synchronous_settings\"] = \"1\"\n",
    "            # insert_quorum = auto:\n",
    "            #   - \"INSERT succeeds only when ClickHouse manages to correctly write data to the insert_quorum of replicas during\n",
    "            #       the insert_quorum_timeout\"\n",
    "            #   - \"use majority number (number_of_replicas / 2 + 1) as quorum number\"\n",
    "            settings[\"insert_quorum\"] = \"auto\"\n",
    "\n",
    "        return {\n",
    "            \"compress\": compress,\n",
    "            \"client_name\": f\"SQLMesh/{__version__}\",\n",
    "            **settings,\n",
    "        }\n",
    "\n",
    "\n",
    "class AthenaConnectionConfig(ConnectionConfig):\n",
    "    # PyAthena connection options\n",
    "    aws_access_key_id: t.Optional[str] = None\n",
    "    aws_secret_access_key: t.Optional[str] = None\n",
    "    role_arn: t.Optional[str] = None\n",
    "    role_session_name: t.Optional[str] = None\n",
    "    region_name: t.Optional[str] = None\n",
    "    work_group: t.Optional[str] = None\n",
    "    s3_staging_dir: t.Optional[str] = None\n",
    "    schema_name: t.Optional[str] = None\n",
    "    catalog_name: t.Optional[str] = None\n",
    "\n",
    "    # SQLMesh options\n",
    "    s3_warehouse_location: t.Optional[str] = None\n",
    "    concurrent_tasks: int = 4\n",
    "    register_comments: t.Literal[False] = (\n",
    "        False  # because Athena doesnt support comments in most cases\n",
    "    )\n",
    "    pre_ping: t.Literal[False] = False\n",
    "\n",
    "    type_: t.Literal[\"athena\"] = Field(alias=\"type\", default=\"athena\")\n",
    "\n",
    "    _engine_import_validator = _get_engine_import_validator(\"pyathena\", \"athena\")\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def _root_validator(self) -> Self:\n",
    "        work_group = self.work_group\n",
    "        s3_staging_dir = self.s3_staging_dir\n",
    "        s3_warehouse_location = self.s3_warehouse_location\n",
    "\n",
    "        if not work_group and not s3_staging_dir:\n",
    "            raise ConfigError(\"At least one of work_group or s3_staging_dir must be set\")\n",
    "\n",
    "        if s3_staging_dir:\n",
    "            self.s3_staging_dir = validate_s3_uri(s3_staging_dir, base=True, error_type=ConfigError)\n",
    "\n",
    "        if s3_warehouse_location:\n",
    "            self.s3_warehouse_location = validate_s3_uri(\n",
    "                s3_warehouse_location, base=True, error_type=ConfigError\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        return {\n",
    "            \"aws_access_key_id\",\n",
    "            \"aws_secret_access_key\",\n",
    "            \"role_arn\",\n",
    "            \"role_session_name\",\n",
    "            \"region_name\",\n",
    "            \"work_group\",\n",
    "            \"s3_staging_dir\",\n",
    "            \"schema_name\",\n",
    "            \"catalog_name\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        return engine_adapter.AthenaEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _extra_engine_config(self) -> t.Dict[str, t.Any]:\n",
    "        return {\"s3_warehouse_location\": self.s3_warehouse_location}\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        from pyathena import connect  # type: ignore\n",
    "\n",
    "        return connect\n",
    "\n",
    "    def get_catalog(self) -> t.Optional[str]:\n",
    "        return self.catalog_name\n",
    "\n",
    "\n",
    "class RisingwaveConnectionConfig(ConnectionConfig):\n",
    "    host: str\n",
    "    user: str\n",
    "    password: t.Optional[str] = None\n",
    "    port: int\n",
    "    database: str\n",
    "    role: t.Optional[str] = None\n",
    "    sslmode: t.Optional[str] = None\n",
    "\n",
    "    concurrent_tasks: int = 4\n",
    "    register_comments: bool = True\n",
    "    pre_ping: bool = True\n",
    "\n",
    "    type_: t.Literal[\"risingwave\"] = Field(alias=\"type\", default=\"risingwave\")\n",
    "\n",
    "    _engine_import_validator = _get_engine_import_validator(\"psycopg2\", \"risingwave\")\n",
    "\n",
    "    @property\n",
    "    def _connection_kwargs_keys(self) -> t.Set[str]:\n",
    "        return {\n",
    "            \"host\",\n",
    "            \"user\",\n",
    "            \"password\",\n",
    "            \"port\",\n",
    "            \"database\",\n",
    "            \"role\",\n",
    "            \"sslmode\",\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _engine_adapter(self) -> t.Type[EngineAdapter]:\n",
    "        return engine_adapter.RisingwaveEngineAdapter\n",
    "\n",
    "    @property\n",
    "    def _connection_factory(self) -> t.Callable:\n",
    "        from psycopg2 import connect\n",
    "\n",
    "        return connect\n",
    "\n",
    "    @property\n",
    "    def _cursor_init(self) -> t.Optional[t.Callable[[t.Any], None]]:\n",
    "        def init(cursor: t.Any) -> None:\n",
    "            sql = \"SET RW_IMPLICIT_FLUSH TO true;\"\n",
    "            cursor.execute(sql)\n",
    "\n",
    "        return init\n",
    "\n",
    "\n",
    "CONNECTION_CONFIG_TO_TYPE = {\n",
    "    # Map all subclasses of ConnectionConfig to the value of their `type_` field.\n",
    "    tpe.all_field_infos()[\"type_\"].default: tpe\n",
    "    for tpe in subclasses(\n",
    "        __name__,\n",
    "        ConnectionConfig,\n",
    "        exclude=(ConnectionConfig, BaseDuckDBConnectionConfig),\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def parse_connection_config(v: t.Dict[str, t.Any]) -> ConnectionConfig:\n",
    "    if \"type\" not in v:\n",
    "        raise ConfigError(\"Missing connection type.\")\n",
    "\n",
    "    connection_type = v[\"type\"]\n",
    "    if connection_type not in CONNECTION_CONFIG_TO_TYPE:\n",
    "        raise ConfigError(f\"Unknown connection type '{connection_type}'.\")\n",
    "\n",
    "    return CONNECTION_CONFIG_TO_TYPE[connection_type](**v)\n",
    "\n",
    "\n",
    "def _connection_config_validator(\n",
    "    cls: t.Type, v: ConnectionConfig | t.Dict[str, t.Any] | None\n",
    ") -> ConnectionConfig | None:\n",
    "    if v is None or isinstance(v, ConnectionConfig):\n",
    "        return v\n",
    "\n",
    "    check_config_and_vars_msg = \"\\n\\nVerify your config.yaml and environment variables.\"\n",
    "\n",
    "    try:\n",
    "        return parse_connection_config(v)\n",
    "    except pydantic.ValidationError as e:\n",
    "        raise ConfigError(\n",
    "            validation_error_message(e, f\"Invalid '{v['type']}' connection config:\")\n",
    "            + check_config_and_vars_msg\n",
    "        )\n",
    "    except ConfigError as e:\n",
    "        raise ConfigError(str(e) + check_config_and_vars_msg)\n",
    "\n",
    "\n",
    "connection_config_validator: t.Callable = field_validator(\n",
    "    \"connection\",\n",
    "    \"state_connection\",\n",
    "    \"test_connection\",\n",
    "    \"default_connection\",\n",
    "    \"default_test_connection\",\n",
    "    mode=\"before\",\n",
    "    check_fields=False,\n",
    ")(_connection_config_validator)\n",
    "\n",
    "\n",
    "if t.TYPE_CHECKING:\n",
    "    # TypeAlias hasn't been introduced until Python 3.10 which means that we can't use it\n",
    "    # outside the TYPE_CHECKING guard.\n",
    "    SerializableConnectionConfig: t.TypeAlias = ConnectionConfig  # type: ignore\n",
    "else:\n",
    "    import pydantic\n",
    "\n",
    "    # Workaround for https://docs.pydantic.dev/latest/concepts/serialization/#serializing-with-duck-typing\n",
    "    SerializableConnectionConfig = pydantic.SerializeAsAny[ConnectionConfig]  # type: ignore\n",
    "'''.strip()\n",
    "\n",
    "\n",
    "with open(\"/home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages/sqlmesh/core/config/connection.py\", \"w\") as f:\n",
    "    f.write(patched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc05d09-4f08-4612-b74b-f0ca4aa6b437",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "!sqlmesh -p /lakehouse/default/Files/{FABRIC_SQLMESH_CODE_PATH} plan --auto-apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a0284-4bdd-4215-9086-af0641d37807",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import fsspec\n",
    "\n",
    "storage_options = {\n",
    "    \"account_name\": \"onelake\",\n",
    "    \"account_host\": \"onelake.blob.fabric.microsoft.com\",\n",
    "    \"anon\": False,\n",
    "}\n",
    "\n",
    "fs = fsspec.filesystem(\"abfs\", **storage_options)\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "con.register_filesystem(fs)\n",
    "\n",
    "host = os.environ.get(\"PG__HOST\")\n",
    "database = os.environ.get(\"PG__DATABASE\")\n",
    "user = os.environ.get(\"PG__USER\")\n",
    "password = os.environ.get(\"PG__PASSWORD\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    INSTALL ducklake;\n",
    "    ATTACH 'ducklake:postgres:\n",
    "        host={host}\n",
    "        dbname={database}\n",
    "        user={user}\n",
    "        password={password}'\n",
    "    AS ducklake\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf70af-392c-49c2-968b-9557a5591128",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "from deltalake import write_deltalake\n",
    "\n",
    "tables = con.sql(\"\"\"\n",
    "    USE ducklake; \n",
    "    SELECT\n",
    "        table_catalog,\n",
    "        table_schema,\n",
    "        table_name\n",
    "    FROM INFORMATION_SCHEMA.TABLES \n",
    "    WHERE\n",
    "        table_schema IN ('silver','gold')\n",
    "    \"\"\").pl().to_dicts()\n",
    "\n",
    "\n",
    "for table in tables:\n",
    "    \n",
    "    catalog = table.get(\"table_catalog\")\n",
    "    schema = table.get(\"table_schema\")\n",
    "    name = table.get(\"table_name\")\n",
    "\n",
    "    print(f\"Syncing ducklake table: {catalog}.{schema}.{name} to delta table: {schema}.{name}\")\n",
    "\n",
    "    data = con.execute(f\"SELECT * FROM {catalog}.{schema}.{name}\").arrow()\n",
    "\n",
    "    write_deltalake(\n",
    "        table_or_uri=f\"{FABRIC_LAKEHOUSE_TABLES_BASE_PATH}/{schema}/{name}\",\n",
    "        data=data,\n",
    "        mode=\"overwrite\",\n",
    "        schema_mode=\"overwrite\",\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1944f-b4de-4397-a248-ccfbd1fb3de6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "4fefaa0f8ce74d9eb9a1fb6306bc43c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "bar_color": "black",
       "description_width": ""
      }
     },
     "8dd78887c59a4775b2d1e289ac39b95c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_b8eee2a93cf04c08a610bc9f129eedca",
       "style": "IPY_MODEL_4fefaa0f8ce74d9eb9a1fb6306bc43c4",
       "value": 100
      }
     },
     "b8eee2a93cf04c08a610bc9f129eedca": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
